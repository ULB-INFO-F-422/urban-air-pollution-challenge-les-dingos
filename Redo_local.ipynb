{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"le package 'randomForest' a été compilé avec la version R 4.3.3\"\n",
      "randomForest 4.7-1.1\n",
      "\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "\n",
      "Attachement du package : 'randomForest'\n",
      "\n",
      "\n",
      "L'objet suivant est masqué depuis 'package:ggplot2':\n",
      "\n",
      "    margin\n",
      "\n",
      "\n",
      "L'objet suivant est masqué depuis 'package:dplyr':\n",
      "\n",
      "    combine\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(caret)\n",
    "library(keras)\n",
    "library(tensorflow)\n",
    "library(randomForest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Function to process training data\n",
    "process_train_data <- function(file_path) {\n",
    "  train_data <- read.csv(file_path)\n",
    "  # Drop the specified columns\n",
    "  train_data <- train_data %>% \n",
    "    select(-target_min, -target_max, -target_variance, -target_count, -Place_ID.X.Date)\n",
    "  return(train_data)\n",
    "}\n",
    "\n",
    "# Function to process test data\n",
    "process_test_data <- function(file_path) {\n",
    "  test_data <- read.csv(file_path)\n",
    "  # Drop the specified column\n",
    "  test_data <- test_data %>%\n",
    "    select(-\"Place_ID.X.Date\")\n",
    "  return(test_data)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Exploration\n",
    "Omitted for now as it has been done in the previous file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Function to remove columns with more than 30% missing values\n",
    "clean_data <- function(data, threshold = 30) {\n",
    "  # Calculate the percentage of missing values for each column\n",
    "  missing_percent <- colSums(is.na(data)) / nrow(data) * 100\n",
    "    \n",
    "  # Identify columns to retain (those with less than the threshold percentage of missing data)\n",
    "  cols_to_keep <- names(data)[missing_percent < threshold]\n",
    "  \n",
    "  # Identify columns to drop\n",
    "  cols_to_drop <- names(data)[missing_percent >= threshold]\n",
    "  \n",
    "  # Select these columns from the data\n",
    "  data_cleaned <- data[, cols_to_keep]\n",
    "  \n",
    "  # Return a list containing the cleaned data and the names of the dropped columns\n",
    "  return(list(cleaned_data = data_cleaned, dropped_columns = cols_to_drop))\n",
    "}\n",
    "\n",
    "#df_no30 <- clean_data(train_data)\n",
    "#dim(df_no30$cleaned_data)\n",
    "#head(df_no30$dropped_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Function to drop columns based on a list of column names, will be used to remove all columsn that will have been dropped from train data frame\n",
    "drop_columns <- function(data, columns_to_drop) {\n",
    "  # Drop the specified columns\n",
    "  data_cleaned <- data %>%\n",
    "    select(-all_of(columns_to_drop))\n",
    "  return(data_cleaned)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#zero_to_na <- function(data) {\n",
    "  # Apply transformation only to numeric columns\n",
    "#  data_no0 <- data\n",
    "#  data_no0[data == 0] <- NA\n",
    "#  return(data_no0)\n",
    "#}\n",
    "\n",
    "zero_to_na <- function(data) {\n",
    "  # Apply transformation only to numeric columns\n",
    "  data <- data %>%\n",
    "    mutate(across(where(is.numeric), ~ replace(., . == 0, NA)))\n",
    "  return(data)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Imputation using the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Function to impute missing values with the mean of each column\n",
    "impute_with_mean <- function(data) {\n",
    "  data <- as.data.frame(data)\n",
    "  for (col in names(data)) {\n",
    "    if (is.numeric(data[[col]])) {\n",
    "      mean_value <- mean(data[[col]], na.rm = TRUE)\n",
    "      data[[col]][is.na(data[[col]])] <- mean_value\n",
    "    }\n",
    "  }\n",
    "  return(data)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# function for min-max scaling and applying it to all numeric columns\n",
    "min_max_scale_data <- function(data) {\n",
    "  # Function to perform min-max scaling\n",
    "  min_max_scale <- function(x) {\n",
    "    return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))\n",
    "  }\n",
    "  \n",
    "  # Apply min-max scaling to all numeric columns\n",
    "  data_min_max_scaled <- data %>%\n",
    "    mutate(across(where(is.numeric), min_max_scale))\n",
    "  \n",
    "  return(data_min_max_scaled)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Randomise Stations for test and train\n",
    "get_train_test <- function(data,rep) {\n",
    "    unique_place_ids <- unique(data$Place_ID)\n",
    "    shuffled_place_ids <- sample(unique_place_ids)\n",
    "    unique_place_ids <- unique(data$Place_ID)\n",
    "    shuffled_place_ids <- sample(unique_place_ids)\n",
    "    num_train <- round(length(shuffled_place_ids) * rep)\n",
    "    train_ids <- shuffled_place_ids[1:num_train]\n",
    "    validation_ids <- shuffled_place_ids[(num_train + 1):length(shuffled_place_ids)]\n",
    "    train <- data[data$Place_ID %in% train_ids, ] %>% \n",
    "      select(-\"Place_ID\")\n",
    "    validation <- data[data$Place_ID %in% validation_ids, ] %>% \n",
    "      select(-\"Place_ID\")\n",
    "    return(list(train = train, validation = validation))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to create and train the neural network\n",
    "train_neural_network <- function(train_data, test_data, target_column, epochs = 100, batch_size = 32) {\n",
    "  # Split the training data into training and validation sets\n",
    "  set.seed(123)  # for reproducibility\n",
    "  train_index <- createDataPartition(y = train_data[[target_column]], p = 0.8, list = FALSE)\n",
    "  train_set <- train_data[train_index, ]\n",
    "  val_set <- train_data[-train_index, ]\n",
    "  \n",
    "  # Extract features and target\n",
    "  train_x <- as.matrix(train_set %>% select(-all_of(target_column)))\n",
    "  train_y <- as.matrix(train_set[[target_column]])\n",
    "  val_x <- as.matrix(val_set %>% select(-all_of(target_column)))\n",
    "  val_y <- as.matrix(val_set[[target_column]])\n",
    "  \n",
    "  # Define the neural network model\n",
    "  model <- keras_model_sequential() %>%\n",
    "    layer_dense(units = 64, activation = 'relu', input_shape = ncol(train_x)) %>%\n",
    "    layer_dense(units = 32, activation = 'relu') %>%\n",
    "    layer_dense(units = 1)\n",
    "  \n",
    "  # Compile the model\n",
    "  model %>% compile(\n",
    "    loss = 'mean_squared_error',\n",
    "    optimizer = optimizer_adam(),\n",
    "    metrics = c('mean_absolute_error')\n",
    "  )\n",
    "  \n",
    "  # Train the model\n",
    "  history <- model %>% fit(\n",
    "    train_x, train_y,\n",
    "    epochs = epochs,\n",
    "    batch_size = batch_size,\n",
    "    validation_data = list(val_x, val_y)\n",
    "  )\n",
    "  \n",
    "  # Evaluate the model on the test data\n",
    "  test_x <- as.matrix(test_data %>% select(-all_of(target_column)))\n",
    "  test_y <- as.matrix(test_data[[target_column]])\n",
    "  evaluation <- model %>% evaluate(test_x, test_y)\n",
    "  \n",
    "  # Make predictions on the test data\n",
    "  predictions <- model %>% predict(test_x)\n",
    "  \n",
    "  # Return the model, evaluation metrics, and predictions\n",
    "  return(list(model = model, evaluation = evaluation, predictions = predictions))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_df <- process_train_data(\"Train.csv\")\n",
    "test_df <- process_test_data(\"Test.csv\")\n",
    "\n",
    "train_df <- zero_to_na(train_df)\n",
    "test_df <- zero_to_na(test_df)\n",
    "\n",
    "train_df <- clean_data(train_df)\n",
    "test_df <- drop_columns(test_df, train_df$dropped_columns )\n",
    "\n",
    "train_df <- impute_with_mean(train_df$cleaned_data)\n",
    "test_df <- impute_with_mean(test_df)\n",
    "\n",
    "train_df <- impute_with_mean(train_df)\n",
    "test_df <- impute_with_mean(test_df)\n",
    "\n",
    "#skip normalisation\n",
    "pre_train <- get_train_test(test_df, 0,8)\n",
    "train_df <- pre_train$train\n",
    "validation_df <- pre_train$validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 68</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Date</th><th scope=col>Place_ID</th><th scope=col>precipitable_water_entire_atmosphere</th><th scope=col>relative_humidity_2m_above_ground</th><th scope=col>specific_humidity_2m_above_ground</th><th scope=col>temperature_2m_above_ground</th><th scope=col>u_component_of_wind_10m_above_ground</th><th scope=col>v_component_of_wind_10m_above_ground</th><th scope=col>L3_NO2_NO2_column_number_density</th><th scope=col>L3_NO2_NO2_slant_column_number_density</th><th scope=col>⋯</th><th scope=col>L3_AER_AI_solar_zenith_angle</th><th scope=col>L3_SO2_SO2_column_number_density</th><th scope=col>L3_SO2_SO2_column_number_density_amf</th><th scope=col>L3_SO2_SO2_slant_column_number_density</th><th scope=col>L3_SO2_absorbing_aerosol_index</th><th scope=col>L3_SO2_cloud_fraction</th><th scope=col>L3_SO2_sensor_azimuth_angle</th><th scope=col>L3_SO2_sensor_zenith_angle</th><th scope=col>L3_SO2_solar_azimuth_angle</th><th scope=col>L3_SO2_solar_zenith_angle</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>2020-01-02</td><td>0OS9LVX</td><td>11.60000</td><td>30.2</td><td>0.00409000</td><td>14.65682</td><td>3.956377</td><td> 0.7126049</td><td>5.338188e-05</td><td>0.0001081872</td><td>⋯</td><td>22.94202</td><td>2.207098e-04</td><td>0.7844364</td><td>1.835919e-04</td><td>-0.1404579</td><td>0.032070799</td><td>  68.09937</td><td> 1.445658</td><td>-95.98498</td><td>22.94202</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>2020-01-03</td><td>0OS9LVX</td><td>18.30000</td><td>42.9</td><td>0.00595000</td><td>15.02654</td><td>4.230430</td><td> 0.6618921</td><td>5.044761e-05</td><td>0.0001090962</td><td>⋯</td><td>18.53952</td><td>3.386942e-05</td><td>0.6789883</td><td>1.353507e-05</td><td>-0.8427128</td><td>0.040803427</td><td>  75.93681</td><td>34.641758</td><td>-95.01491</td><td>18.53912</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>2020-01-04</td><td>0OS9LVX</td><td>17.60000</td><td>41.3</td><td>0.00590000</td><td>15.51104</td><td>5.245728</td><td> 1.6405591</td><td>5.035383e-05</td><td>0.0001344593</td><td>⋯</td><td>14.14082</td><td>1.839346e-04</td><td>0.6677681</td><td>1.219163e-04</td><td>-0.7167696</td><td>0.007112971</td><td>  75.55244</td><td>55.872276</td><td>-94.01542</td><td>14.14082</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>2020-01-05</td><td>0OS9LVX</td><td>15.01195</td><td>53.1</td><td>0.00709000</td><td>14.44186</td><td>5.454001</td><td>-0.1905322</td><td>5.499153e-05</td><td>0.0001546277</td><td>⋯</td><td>32.73075</td><td>2.008748e-04</td><td>0.6967723</td><td>1.328754e-04</td><td>-0.7301036</td><td>0.062076021</td><td>-102.28513</td><td>59.174188</td><td>-97.24760</td><td>32.73055</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>2020-01-06</td><td>0OS9LVX</td><td> 9.70000</td><td>71.6</td><td>0.00808000</td><td>11.89630</td><td>3.511787</td><td>-0.2794409</td><td>5.508028e-05</td><td>0.0001308539</td><td>⋯</td><td>28.32053</td><td>9.338975e-05</td><td>0.6773054</td><td>6.526173e-05</td><td>-0.1083527</td><td>0.042776815</td><td>-102.13396</td><td>40.925873</td><td>-96.05727</td><td>28.32053</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>2020-01-07</td><td>0OS9LVX</td><td>13.40000</td><td>69.3</td><td>0.00828156</td><td>12.74487</td><td>3.945603</td><td>-0.2409253</td><td>5.809714e-05</td><td>0.0001141703</td><td>⋯</td><td>23.90786</td><td>8.664031e-05</td><td>0.7463363</td><td>4.455912e-05</td><td> 0.2506700</td><td>0.039987010</td><td>-102.89442</td><td>10.836973</td><td>-94.79964</td><td>23.90786</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 68\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & Date & Place\\_ID & precipitable\\_water\\_entire\\_atmosphere & relative\\_humidity\\_2m\\_above\\_ground & specific\\_humidity\\_2m\\_above\\_ground & temperature\\_2m\\_above\\_ground & u\\_component\\_of\\_wind\\_10m\\_above\\_ground & v\\_component\\_of\\_wind\\_10m\\_above\\_ground & L3\\_NO2\\_NO2\\_column\\_number\\_density & L3\\_NO2\\_NO2\\_slant\\_column\\_number\\_density & ⋯ & L3\\_AER\\_AI\\_solar\\_zenith\\_angle & L3\\_SO2\\_SO2\\_column\\_number\\_density & L3\\_SO2\\_SO2\\_column\\_number\\_density\\_amf & L3\\_SO2\\_SO2\\_slant\\_column\\_number\\_density & L3\\_SO2\\_absorbing\\_aerosol\\_index & L3\\_SO2\\_cloud\\_fraction & L3\\_SO2\\_sensor\\_azimuth\\_angle & L3\\_SO2\\_sensor\\_zenith\\_angle & L3\\_SO2\\_solar\\_azimuth\\_angle & L3\\_SO2\\_solar\\_zenith\\_angle\\\\\n",
       "  & <chr> & <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 2020-01-02 & 0OS9LVX & 11.60000 & 30.2 & 0.00409000 & 14.65682 & 3.956377 &  0.7126049 & 5.338188e-05 & 0.0001081872 & ⋯ & 22.94202 & 2.207098e-04 & 0.7844364 & 1.835919e-04 & -0.1404579 & 0.032070799 &   68.09937 &  1.445658 & -95.98498 & 22.94202\\\\\n",
       "\t2 & 2020-01-03 & 0OS9LVX & 18.30000 & 42.9 & 0.00595000 & 15.02654 & 4.230430 &  0.6618921 & 5.044761e-05 & 0.0001090962 & ⋯ & 18.53952 & 3.386942e-05 & 0.6789883 & 1.353507e-05 & -0.8427128 & 0.040803427 &   75.93681 & 34.641758 & -95.01491 & 18.53912\\\\\n",
       "\t3 & 2020-01-04 & 0OS9LVX & 17.60000 & 41.3 & 0.00590000 & 15.51104 & 5.245728 &  1.6405591 & 5.035383e-05 & 0.0001344593 & ⋯ & 14.14082 & 1.839346e-04 & 0.6677681 & 1.219163e-04 & -0.7167696 & 0.007112971 &   75.55244 & 55.872276 & -94.01542 & 14.14082\\\\\n",
       "\t4 & 2020-01-05 & 0OS9LVX & 15.01195 & 53.1 & 0.00709000 & 14.44186 & 5.454001 & -0.1905322 & 5.499153e-05 & 0.0001546277 & ⋯ & 32.73075 & 2.008748e-04 & 0.6967723 & 1.328754e-04 & -0.7301036 & 0.062076021 & -102.28513 & 59.174188 & -97.24760 & 32.73055\\\\\n",
       "\t5 & 2020-01-06 & 0OS9LVX &  9.70000 & 71.6 & 0.00808000 & 11.89630 & 3.511787 & -0.2794409 & 5.508028e-05 & 0.0001308539 & ⋯ & 28.32053 & 9.338975e-05 & 0.6773054 & 6.526173e-05 & -0.1083527 & 0.042776815 & -102.13396 & 40.925873 & -96.05727 & 28.32053\\\\\n",
       "\t6 & 2020-01-07 & 0OS9LVX & 13.40000 & 69.3 & 0.00828156 & 12.74487 & 3.945603 & -0.2409253 & 5.809714e-05 & 0.0001141703 & ⋯ & 23.90786 & 8.664031e-05 & 0.7463363 & 4.455912e-05 &  0.2506700 & 0.039987010 & -102.89442 & 10.836973 & -94.79964 & 23.90786\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 68\n",
       "\n",
       "| <!--/--> | Date &lt;chr&gt; | Place_ID &lt;chr&gt; | precipitable_water_entire_atmosphere &lt;dbl&gt; | relative_humidity_2m_above_ground &lt;dbl&gt; | specific_humidity_2m_above_ground &lt;dbl&gt; | temperature_2m_above_ground &lt;dbl&gt; | u_component_of_wind_10m_above_ground &lt;dbl&gt; | v_component_of_wind_10m_above_ground &lt;dbl&gt; | L3_NO2_NO2_column_number_density &lt;dbl&gt; | L3_NO2_NO2_slant_column_number_density &lt;dbl&gt; | ⋯ ⋯ | L3_AER_AI_solar_zenith_angle &lt;dbl&gt; | L3_SO2_SO2_column_number_density &lt;dbl&gt; | L3_SO2_SO2_column_number_density_amf &lt;dbl&gt; | L3_SO2_SO2_slant_column_number_density &lt;dbl&gt; | L3_SO2_absorbing_aerosol_index &lt;dbl&gt; | L3_SO2_cloud_fraction &lt;dbl&gt; | L3_SO2_sensor_azimuth_angle &lt;dbl&gt; | L3_SO2_sensor_zenith_angle &lt;dbl&gt; | L3_SO2_solar_azimuth_angle &lt;dbl&gt; | L3_SO2_solar_zenith_angle &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 2020-01-02 | 0OS9LVX | 11.60000 | 30.2 | 0.00409000 | 14.65682 | 3.956377 |  0.7126049 | 5.338188e-05 | 0.0001081872 | ⋯ | 22.94202 | 2.207098e-04 | 0.7844364 | 1.835919e-04 | -0.1404579 | 0.032070799 |   68.09937 |  1.445658 | -95.98498 | 22.94202 |\n",
       "| 2 | 2020-01-03 | 0OS9LVX | 18.30000 | 42.9 | 0.00595000 | 15.02654 | 4.230430 |  0.6618921 | 5.044761e-05 | 0.0001090962 | ⋯ | 18.53952 | 3.386942e-05 | 0.6789883 | 1.353507e-05 | -0.8427128 | 0.040803427 |   75.93681 | 34.641758 | -95.01491 | 18.53912 |\n",
       "| 3 | 2020-01-04 | 0OS9LVX | 17.60000 | 41.3 | 0.00590000 | 15.51104 | 5.245728 |  1.6405591 | 5.035383e-05 | 0.0001344593 | ⋯ | 14.14082 | 1.839346e-04 | 0.6677681 | 1.219163e-04 | -0.7167696 | 0.007112971 |   75.55244 | 55.872276 | -94.01542 | 14.14082 |\n",
       "| 4 | 2020-01-05 | 0OS9LVX | 15.01195 | 53.1 | 0.00709000 | 14.44186 | 5.454001 | -0.1905322 | 5.499153e-05 | 0.0001546277 | ⋯ | 32.73075 | 2.008748e-04 | 0.6967723 | 1.328754e-04 | -0.7301036 | 0.062076021 | -102.28513 | 59.174188 | -97.24760 | 32.73055 |\n",
       "| 5 | 2020-01-06 | 0OS9LVX |  9.70000 | 71.6 | 0.00808000 | 11.89630 | 3.511787 | -0.2794409 | 5.508028e-05 | 0.0001308539 | ⋯ | 28.32053 | 9.338975e-05 | 0.6773054 | 6.526173e-05 | -0.1083527 | 0.042776815 | -102.13396 | 40.925873 | -96.05727 | 28.32053 |\n",
       "| 6 | 2020-01-07 | 0OS9LVX | 13.40000 | 69.3 | 0.00828156 | 12.74487 | 3.945603 | -0.2409253 | 5.809714e-05 | 0.0001141703 | ⋯ | 23.90786 | 8.664031e-05 | 0.7463363 | 4.455912e-05 |  0.2506700 | 0.039987010 | -102.89442 | 10.836973 | -94.79964 | 23.90786 |\n",
       "\n"
      ],
      "text/plain": [
       "  Date       Place_ID precipitable_water_entire_atmosphere\n",
       "1 2020-01-02 0OS9LVX  11.60000                            \n",
       "2 2020-01-03 0OS9LVX  18.30000                            \n",
       "3 2020-01-04 0OS9LVX  17.60000                            \n",
       "4 2020-01-05 0OS9LVX  15.01195                            \n",
       "5 2020-01-06 0OS9LVX   9.70000                            \n",
       "6 2020-01-07 0OS9LVX  13.40000                            \n",
       "  relative_humidity_2m_above_ground specific_humidity_2m_above_ground\n",
       "1 30.2                              0.00409000                       \n",
       "2 42.9                              0.00595000                       \n",
       "3 41.3                              0.00590000                       \n",
       "4 53.1                              0.00709000                       \n",
       "5 71.6                              0.00808000                       \n",
       "6 69.3                              0.00828156                       \n",
       "  temperature_2m_above_ground u_component_of_wind_10m_above_ground\n",
       "1 14.65682                    3.956377                            \n",
       "2 15.02654                    4.230430                            \n",
       "3 15.51104                    5.245728                            \n",
       "4 14.44186                    5.454001                            \n",
       "5 11.89630                    3.511787                            \n",
       "6 12.74487                    3.945603                            \n",
       "  v_component_of_wind_10m_above_ground L3_NO2_NO2_column_number_density\n",
       "1  0.7126049                           5.338188e-05                    \n",
       "2  0.6618921                           5.044761e-05                    \n",
       "3  1.6405591                           5.035383e-05                    \n",
       "4 -0.1905322                           5.499153e-05                    \n",
       "5 -0.2794409                           5.508028e-05                    \n",
       "6 -0.2409253                           5.809714e-05                    \n",
       "  L3_NO2_NO2_slant_column_number_density ⋯ L3_AER_AI_solar_zenith_angle\n",
       "1 0.0001081872                           ⋯ 22.94202                    \n",
       "2 0.0001090962                           ⋯ 18.53952                    \n",
       "3 0.0001344593                           ⋯ 14.14082                    \n",
       "4 0.0001546277                           ⋯ 32.73075                    \n",
       "5 0.0001308539                           ⋯ 28.32053                    \n",
       "6 0.0001141703                           ⋯ 23.90786                    \n",
       "  L3_SO2_SO2_column_number_density L3_SO2_SO2_column_number_density_amf\n",
       "1 2.207098e-04                     0.7844364                           \n",
       "2 3.386942e-05                     0.6789883                           \n",
       "3 1.839346e-04                     0.6677681                           \n",
       "4 2.008748e-04                     0.6967723                           \n",
       "5 9.338975e-05                     0.6773054                           \n",
       "6 8.664031e-05                     0.7463363                           \n",
       "  L3_SO2_SO2_slant_column_number_density L3_SO2_absorbing_aerosol_index\n",
       "1 1.835919e-04                           -0.1404579                    \n",
       "2 1.353507e-05                           -0.8427128                    \n",
       "3 1.219163e-04                           -0.7167696                    \n",
       "4 1.328754e-04                           -0.7301036                    \n",
       "5 6.526173e-05                           -0.1083527                    \n",
       "6 4.455912e-05                            0.2506700                    \n",
       "  L3_SO2_cloud_fraction L3_SO2_sensor_azimuth_angle L3_SO2_sensor_zenith_angle\n",
       "1 0.032070799             68.09937                   1.445658                 \n",
       "2 0.040803427             75.93681                  34.641758                 \n",
       "3 0.007112971             75.55244                  55.872276                 \n",
       "4 0.062076021           -102.28513                  59.174188                 \n",
       "5 0.042776815           -102.13396                  40.925873                 \n",
       "6 0.039987010           -102.89442                  10.836973                 \n",
       "  L3_SO2_solar_azimuth_angle L3_SO2_solar_zenith_angle\n",
       "1 -95.98498                  22.94202                 \n",
       "2 -95.01491                  18.53912                 \n",
       "3 -94.01542                  14.14082                 \n",
       "4 -97.24760                  32.73055                 \n",
       "5 -96.05727                  28.32053                 \n",
       "6 -94.79964                  23.90786                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: <Sequential name=sequential, built=False> (of type <class 'keras.src.models.sequential.Sequential'>)",
     "output_type": "error",
     "traceback": [
      "Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: <Sequential name=sequential, built=False> (of type <class 'keras.src.models.sequential.Sequential'>)Traceback:\n",
      "1. train_neural_network(train_df, validation_df, \"target\")",
      "2. keras_model_sequential() %>% layer_dense(units = 64, activation = \"relu\", \n .     input_shape = ncol(train_x)) %>% layer_dense(units = 32, \n .     activation = \"relu\") %>% layer_dense(units = 1)   # at line 16-19 of file <text>",
      "3. layer_dense(., units = 1)",
      "4. create_layer(keras$layers$Dense, object, list(units = as.integer(units), \n .     activation = activation, use_bias = use_bias, kernel_initializer = kernel_initializer, \n .     bias_initializer = bias_initializer, kernel_regularizer = kernel_regularizer, \n .     bias_regularizer = bias_regularizer, activity_regularizer = activity_regularizer, \n .     kernel_constraint = kernel_constraint, bias_constraint = bias_constraint, \n .     input_shape = normalize_shape(input_shape), batch_input_shape = normalize_shape(batch_input_shape), \n .     batch_size = as_nullable_integer(batch_size), dtype = dtype, \n .     name = name, trainable = trainable, weights = weights))",
      "5. layer_dense(., units = 32, activation = \"relu\")",
      "6. create_layer(keras$layers$Dense, object, list(units = as.integer(units), \n .     activation = activation, use_bias = use_bias, kernel_initializer = kernel_initializer, \n .     bias_initializer = bias_initializer, kernel_regularizer = kernel_regularizer, \n .     bias_regularizer = bias_regularizer, activity_regularizer = activity_regularizer, \n .     kernel_constraint = kernel_constraint, bias_constraint = bias_constraint, \n .     input_shape = normalize_shape(input_shape), batch_input_shape = normalize_shape(batch_input_shape), \n .     batch_size = as_nullable_integer(batch_size), dtype = dtype, \n .     name = name, trainable = trainable, weights = weights))",
      "7. layer_dense(., units = 64, activation = \"relu\", input_shape = ncol(train_x))",
      "8. create_layer(keras$layers$Dense, object, list(units = as.integer(units), \n .     activation = activation, use_bias = use_bias, kernel_initializer = kernel_initializer, \n .     bias_initializer = bias_initializer, kernel_regularizer = kernel_regularizer, \n .     bias_regularizer = bias_regularizer, activity_regularizer = activity_regularizer, \n .     kernel_constraint = kernel_constraint, bias_constraint = bias_constraint, \n .     input_shape = normalize_shape(input_shape), batch_input_shape = normalize_shape(batch_input_shape), \n .     batch_size = as_nullable_integer(batch_size), dtype = dtype, \n .     name = name, trainable = trainable, weights = weights))",
      "9. compose_layer(object, layer)",
      "10. compose_layer.default(object, layer)",
      "11. layer(object, ...)",
      "12. py_call_impl(callable, call_args$unnamed, call_args$named)"
     ]
    }
   ],
   "source": [
    "nn_model <- train_neural_network(train_df, validation_df, \"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rf_model <- randomForest(target ~ ., data = train_df, ntree = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Root Mean Squared Error (RMSE): 12.4870265015593\"\n"
     ]
    }
   ],
   "source": [
    "predictions <- predict(rf_model, train_df)\n",
    "rmse <- sqrt(mean((predictions - train_df$target)^2))\n",
    "print(paste(\"Root Mean Squared Error (RMSE):\", rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Predictions saved to Predictions.csv\"\n"
     ]
    }
   ],
   "source": [
    "# Define a function to apply the model to new data and save predictions\n",
    "apply_model_to_data <- function(model, new_data, sample_submission_path, output_path) {\n",
    "  # Read the sample submission file to get the structure\n",
    "  sample_submission <- read.csv(sample_submission_path)\n",
    "  \n",
    "  # Extract features from new data (assume Place_ID column exists and exclude it)\n",
    "  new_data_x <- new_data %>% select(-Place_ID)\n",
    "  \n",
    "  # Generate predictions using the trained model\n",
    "  predictions <- predict(model, new_data_x)\n",
    "  \n",
    "  # Create a new data frame with the predictions\n",
    "  submission <- sample_submission\n",
    "  submission$Prediction <- predictions\n",
    "  \n",
    "  # Save the predictions to a CSV file\n",
    "  write.csv(submission, file = output_path, row.names = FALSE)\n",
    "}\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `rf_model` is your trained Random Forest model and `test_data_scaled` is your new data\n",
    "\n",
    "\n",
    "# Specify the path to the sample submission file and the output path for the predictions\n",
    "sample_submission_path <- \"SampleSubmission.csv\"\n",
    "output_path <- \"Predictions.csv\"\n",
    "\n",
    "# Apply the model to the new data and save predictions\n",
    "apply_model_to_data(rf_model, test_df, sample_submission_path, output_path)\n",
    "\n",
    "# Verify that the predictions file has been created correctly\n",
    "print(\"Predictions saved to Predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
